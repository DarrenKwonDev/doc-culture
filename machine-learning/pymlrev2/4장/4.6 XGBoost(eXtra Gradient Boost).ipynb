{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* XGBoost 버전 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.3\n"
     ]
    }
   ],
   "source": [
    "import xgboost\n",
    "\n",
    "print(xgboost.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파이썬 래퍼 XGBoost 적용 – 위스콘신 유방암 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>17.20</td>\n",
       "      <td>24.52</td>\n",
       "      <td>114.20</td>\n",
       "      <td>929.4</td>\n",
       "      <td>0.10710</td>\n",
       "      <td>0.18300</td>\n",
       "      <td>0.169200</td>\n",
       "      <td>0.079440</td>\n",
       "      <td>0.1927</td>\n",
       "      <td>0.06487</td>\n",
       "      <td>...</td>\n",
       "      <td>33.82</td>\n",
       "      <td>151.60</td>\n",
       "      <td>1681.0</td>\n",
       "      <td>0.1585</td>\n",
       "      <td>0.73940</td>\n",
       "      <td>0.65660</td>\n",
       "      <td>0.18990</td>\n",
       "      <td>0.3313</td>\n",
       "      <td>0.13390</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>17.54</td>\n",
       "      <td>19.32</td>\n",
       "      <td>115.10</td>\n",
       "      <td>951.6</td>\n",
       "      <td>0.08968</td>\n",
       "      <td>0.11980</td>\n",
       "      <td>0.103600</td>\n",
       "      <td>0.074880</td>\n",
       "      <td>0.1506</td>\n",
       "      <td>0.05491</td>\n",
       "      <td>...</td>\n",
       "      <td>25.84</td>\n",
       "      <td>139.50</td>\n",
       "      <td>1239.0</td>\n",
       "      <td>0.1381</td>\n",
       "      <td>0.34200</td>\n",
       "      <td>0.35080</td>\n",
       "      <td>0.19390</td>\n",
       "      <td>0.2928</td>\n",
       "      <td>0.07867</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>11.26</td>\n",
       "      <td>19.83</td>\n",
       "      <td>71.30</td>\n",
       "      <td>388.1</td>\n",
       "      <td>0.08511</td>\n",
       "      <td>0.04413</td>\n",
       "      <td>0.005067</td>\n",
       "      <td>0.005664</td>\n",
       "      <td>0.1637</td>\n",
       "      <td>0.06343</td>\n",
       "      <td>...</td>\n",
       "      <td>26.43</td>\n",
       "      <td>76.38</td>\n",
       "      <td>435.9</td>\n",
       "      <td>0.1108</td>\n",
       "      <td>0.07723</td>\n",
       "      <td>0.02533</td>\n",
       "      <td>0.02832</td>\n",
       "      <td>0.2557</td>\n",
       "      <td>0.07613</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>14.25</td>\n",
       "      <td>21.72</td>\n",
       "      <td>93.63</td>\n",
       "      <td>633.0</td>\n",
       "      <td>0.09823</td>\n",
       "      <td>0.10980</td>\n",
       "      <td>0.131900</td>\n",
       "      <td>0.055980</td>\n",
       "      <td>0.1885</td>\n",
       "      <td>0.06125</td>\n",
       "      <td>...</td>\n",
       "      <td>30.36</td>\n",
       "      <td>116.20</td>\n",
       "      <td>799.6</td>\n",
       "      <td>0.1446</td>\n",
       "      <td>0.42380</td>\n",
       "      <td>0.51860</td>\n",
       "      <td>0.14470</td>\n",
       "      <td>0.3591</td>\n",
       "      <td>0.10140</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>19.10</td>\n",
       "      <td>26.29</td>\n",
       "      <td>129.10</td>\n",
       "      <td>1132.0</td>\n",
       "      <td>0.12150</td>\n",
       "      <td>0.17910</td>\n",
       "      <td>0.193700</td>\n",
       "      <td>0.146900</td>\n",
       "      <td>0.1634</td>\n",
       "      <td>0.07224</td>\n",
       "      <td>...</td>\n",
       "      <td>32.72</td>\n",
       "      <td>141.30</td>\n",
       "      <td>1298.0</td>\n",
       "      <td>0.1392</td>\n",
       "      <td>0.28170</td>\n",
       "      <td>0.24320</td>\n",
       "      <td>0.18410</td>\n",
       "      <td>0.2311</td>\n",
       "      <td>0.09203</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "72         17.20         24.52          114.20      929.4          0.10710   \n",
       "201        17.54         19.32          115.10      951.6          0.08968   \n",
       "522        11.26         19.83           71.30      388.1          0.08511   \n",
       "36         14.25         21.72           93.63      633.0          0.09823   \n",
       "83         19.10         26.29          129.10     1132.0          0.12150   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "72            0.18300        0.169200             0.079440         0.1927   \n",
       "201           0.11980        0.103600             0.074880         0.1506   \n",
       "522           0.04413        0.005067             0.005664         0.1637   \n",
       "36            0.10980        0.131900             0.055980         0.1885   \n",
       "83            0.17910        0.193700             0.146900         0.1634   \n",
       "\n",
       "     mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "72                  0.06487  ...          33.82           151.60      1681.0   \n",
       "201                 0.05491  ...          25.84           139.50      1239.0   \n",
       "522                 0.06343  ...          26.43            76.38       435.9   \n",
       "36                  0.06125  ...          30.36           116.20       799.6   \n",
       "83                  0.07224  ...          32.72           141.30      1298.0   \n",
       "\n",
       "     worst smoothness  worst compactness  worst concavity  \\\n",
       "72             0.1585            0.73940          0.65660   \n",
       "201            0.1381            0.34200          0.35080   \n",
       "522            0.1108            0.07723          0.02533   \n",
       "36             0.1446            0.42380          0.51860   \n",
       "83             0.1392            0.28170          0.24320   \n",
       "\n",
       "     worst concave points  worst symmetry  worst fractal dimension  target  \n",
       "72                0.18990          0.3313                  0.13390       0  \n",
       "201               0.19390          0.2928                  0.07867       0  \n",
       "522               0.02832          0.2557                  0.07613       1  \n",
       "36                0.14470          0.3591                  0.10140       0  \n",
       "83                0.18410          0.2311                  0.09203       0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb # python wrapper\n",
    "from xgboost import plot_importance\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "dataset = load_breast_cancer()\n",
    "X_features= dataset.data\n",
    "y_label = dataset.target\n",
    "\n",
    "cancer_df = pd.DataFrame(data=X_features, columns=dataset.feature_names)\n",
    "cancer_df['target']= y_label\n",
    "\n",
    "cancer_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['malignant' 'benign']\n",
      "1    62.74\n",
      "0    37.26\n",
      "Name: target, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(dataset.target_names)\n",
    "print(np.round(cancer_df['target'].value_counts(normalize=True) * 100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(455, 30) (114, 30)\n",
      "(409, 30) (46, 30)\n"
     ]
    }
   ],
   "source": [
    "# cancer_df에서 feature용 DataFrame과 Label용 Series 객체 추출\n",
    "# 맨 마지막 칼럼이 Label임. Feature용 DataFrame은 cancer_df의 첫번째 칼럼에서 맨 마지막 두번째 칼럼까지를 :-1 슬라이싱으로 추출.\n",
    "X_features = cancer_df.iloc[:, :-1]\n",
    "y_label = cancer_df.iloc[:, -1]\n",
    "\n",
    "# 전체 데이터 중 80%는 학습용 데이터, 20%는 테스트용 데이터 추출\n",
    "X_train, X_test, y_train, y_test=train_test_split(X_features, y_label,\n",
    "                                        test_size=0.2, random_state=156 )\n",
    "\n",
    "# 위에서 만든 X_train, y_train을 다시 쪼개서 90%는 학습과 10%는 검증용 데이터로 분리\n",
    "X_tr, X_val, y_tr, y_val= train_test_split(X_train, y_train, test_size=0.1, random_state=156 )\n",
    "print(X_train.shape , X_test.shape)\n",
    "print(X_tr.shape, X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 만약 구버전 XGBoost에서 DataFrame으로 DMatrix 생성이 안될 경우 X_train.values로 넘파이 변환.\n",
    "\n",
    "# 학습, 검증, 테스트용 DMatrix를 생성.\n",
    "dtr = xgb.DMatrix(data=X_tr, label=y_tr)\n",
    "dval = xgb.DMatrix(data=X_val, label=y_val)\n",
    "dtest = xgb.DMatrix(data=X_test , label=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3] 34 None None\n",
      "[1, 2, 3] 34 test None\n"
     ]
    }
   ],
   "source": [
    "# data, label은 인자 이름 없이 position으로 주어도 알아 들음\n",
    "# 그러나 * 뒤에 나오는 인자는 반드시 인자 이름으로 주어야 함\n",
    "def test(data, label, *, name = None, score = None):\n",
    "    print(data, label, name, score)\n",
    "\n",
    "test([1, 2, 3], 34)\n",
    "test([1, 2, 3], 34, name='test')\n",
    "test([1, 2, 3], 34, 'test') # error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.65016\teval-logloss:0.66183\n",
      "[1]\ttrain-logloss:0.61131\teval-logloss:0.63609\n",
      "[2]\ttrain-logloss:0.57563\teval-logloss:0.61144\n",
      "[3]\ttrain-logloss:0.54310\teval-logloss:0.59204\n",
      "[4]\ttrain-logloss:0.51323\teval-logloss:0.57329\n",
      "[5]\ttrain-logloss:0.48447\teval-logloss:0.55037\n",
      "[6]\ttrain-logloss:0.45796\teval-logloss:0.52930\n",
      "[7]\ttrain-logloss:0.43436\teval-logloss:0.51534\n",
      "[8]\ttrain-logloss:0.41150\teval-logloss:0.49718\n",
      "[9]\ttrain-logloss:0.39027\teval-logloss:0.48154\n",
      "[10]\ttrain-logloss:0.37128\teval-logloss:0.46990\n",
      "[11]\ttrain-logloss:0.35254\teval-logloss:0.45474\n",
      "[12]\ttrain-logloss:0.33528\teval-logloss:0.44229\n",
      "[13]\ttrain-logloss:0.31892\teval-logloss:0.42961\n",
      "[14]\ttrain-logloss:0.30439\teval-logloss:0.42065\n",
      "[15]\ttrain-logloss:0.29000\teval-logloss:0.40958\n",
      "[16]\ttrain-logloss:0.27651\teval-logloss:0.39887\n",
      "[17]\ttrain-logloss:0.26389\teval-logloss:0.39050\n",
      "[18]\ttrain-logloss:0.25210\teval-logloss:0.38254\n",
      "[19]\ttrain-logloss:0.24123\teval-logloss:0.37393\n",
      "[20]\ttrain-logloss:0.23076\teval-logloss:0.36789\n",
      "[21]\ttrain-logloss:0.22091\teval-logloss:0.36017\n",
      "[22]\ttrain-logloss:0.21155\teval-logloss:0.35421\n",
      "[23]\ttrain-logloss:0.20263\teval-logloss:0.34683\n",
      "[24]\ttrain-logloss:0.19434\teval-logloss:0.34111\n",
      "[25]\ttrain-logloss:0.18637\teval-logloss:0.33634\n",
      "[26]\ttrain-logloss:0.17875\teval-logloss:0.33082\n",
      "[27]\ttrain-logloss:0.17167\teval-logloss:0.32675\n",
      "[28]\ttrain-logloss:0.16481\teval-logloss:0.32099\n",
      "[29]\ttrain-logloss:0.15835\teval-logloss:0.31671\n",
      "[30]\ttrain-logloss:0.15225\teval-logloss:0.31277\n",
      "[31]\ttrain-logloss:0.14650\teval-logloss:0.30882\n",
      "[32]\ttrain-logloss:0.14102\teval-logloss:0.30437\n",
      "[33]\ttrain-logloss:0.13590\teval-logloss:0.30103\n",
      "[34]\ttrain-logloss:0.13109\teval-logloss:0.29794\n",
      "[35]\ttrain-logloss:0.12647\teval-logloss:0.29499\n",
      "[36]\ttrain-logloss:0.12197\teval-logloss:0.29295\n",
      "[37]\ttrain-logloss:0.11784\teval-logloss:0.29043\n",
      "[38]\ttrain-logloss:0.11379\teval-logloss:0.28927\n",
      "[39]\ttrain-logloss:0.10994\teval-logloss:0.28578\n",
      "[40]\ttrain-logloss:0.10638\teval-logloss:0.28364\n",
      "[41]\ttrain-logloss:0.10302\teval-logloss:0.28183\n",
      "[42]\ttrain-logloss:0.09963\teval-logloss:0.28005\n",
      "[43]\ttrain-logloss:0.09649\teval-logloss:0.27972\n",
      "[44]\ttrain-logloss:0.09359\teval-logloss:0.27744\n",
      "[45]\ttrain-logloss:0.09080\teval-logloss:0.27542\n",
      "[46]\ttrain-logloss:0.08807\teval-logloss:0.27504\n",
      "[47]\ttrain-logloss:0.08541\teval-logloss:0.27458\n",
      "[48]\ttrain-logloss:0.08299\teval-logloss:0.27348\n",
      "[49]\ttrain-logloss:0.08035\teval-logloss:0.27247\n",
      "[50]\ttrain-logloss:0.07786\teval-logloss:0.27163\n",
      "[51]\ttrain-logloss:0.07550\teval-logloss:0.27094\n",
      "[52]\ttrain-logloss:0.07344\teval-logloss:0.26967\n",
      "[53]\ttrain-logloss:0.07147\teval-logloss:0.27008\n",
      "[54]\ttrain-logloss:0.06964\teval-logloss:0.26890\n",
      "[55]\ttrain-logloss:0.06766\teval-logloss:0.26854\n",
      "[56]\ttrain-logloss:0.06591\teval-logloss:0.26900\n",
      "[57]\ttrain-logloss:0.06433\teval-logloss:0.26790\n",
      "[58]\ttrain-logloss:0.06259\teval-logloss:0.26663\n",
      "[59]\ttrain-logloss:0.06107\teval-logloss:0.26743\n",
      "[60]\ttrain-logloss:0.05957\teval-logloss:0.26610\n",
      "[61]\ttrain-logloss:0.05817\teval-logloss:0.26644\n",
      "[62]\ttrain-logloss:0.05691\teval-logloss:0.26673\n",
      "[63]\ttrain-logloss:0.05550\teval-logloss:0.26550\n",
      "[64]\ttrain-logloss:0.05422\teval-logloss:0.26443\n",
      "[65]\ttrain-logloss:0.05311\teval-logloss:0.26500\n",
      "[66]\ttrain-logloss:0.05207\teval-logloss:0.26591\n",
      "[67]\ttrain-logloss:0.05093\teval-logloss:0.26501\n",
      "[68]\ttrain-logloss:0.04976\teval-logloss:0.26435\n",
      "[69]\ttrain-logloss:0.04872\teval-logloss:0.26360\n",
      "[70]\ttrain-logloss:0.04776\teval-logloss:0.26319\n",
      "[71]\ttrain-logloss:0.04680\teval-logloss:0.26255\n",
      "[72]\ttrain-logloss:0.04580\teval-logloss:0.26204\n",
      "[73]\ttrain-logloss:0.04484\teval-logloss:0.26254\n",
      "[74]\ttrain-logloss:0.04388\teval-logloss:0.26289\n",
      "[75]\ttrain-logloss:0.04309\teval-logloss:0.26249\n",
      "[76]\ttrain-logloss:0.04224\teval-logloss:0.26217\n",
      "[77]\ttrain-logloss:0.04133\teval-logloss:0.26166\n",
      "[78]\ttrain-logloss:0.04050\teval-logloss:0.26179\n",
      "[79]\ttrain-logloss:0.03967\teval-logloss:0.26103\n",
      "[80]\ttrain-logloss:0.03876\teval-logloss:0.26094\n",
      "[81]\ttrain-logloss:0.03806\teval-logloss:0.26148\n",
      "[82]\ttrain-logloss:0.03740\teval-logloss:0.26054\n",
      "[83]\ttrain-logloss:0.03676\teval-logloss:0.25967\n",
      "[84]\ttrain-logloss:0.03605\teval-logloss:0.25905\n",
      "[85]\ttrain-logloss:0.03545\teval-logloss:0.26007\n",
      "[86]\ttrain-logloss:0.03489\teval-logloss:0.25984\n",
      "[87]\ttrain-logloss:0.03425\teval-logloss:0.25933\n",
      "[88]\ttrain-logloss:0.03361\teval-logloss:0.25932\n",
      "[89]\ttrain-logloss:0.03311\teval-logloss:0.26002\n",
      "[90]\ttrain-logloss:0.03260\teval-logloss:0.25936\n",
      "[91]\ttrain-logloss:0.03202\teval-logloss:0.25886\n",
      "[92]\ttrain-logloss:0.03152\teval-logloss:0.25918\n",
      "[93]\ttrain-logloss:0.03107\teval-logloss:0.25864\n",
      "[94]\ttrain-logloss:0.03049\teval-logloss:0.25951\n",
      "[95]\ttrain-logloss:0.03007\teval-logloss:0.26091\n",
      "[96]\ttrain-logloss:0.02963\teval-logloss:0.26014\n",
      "[97]\ttrain-logloss:0.02913\teval-logloss:0.25974\n",
      "[98]\ttrain-logloss:0.02866\teval-logloss:0.25937\n",
      "[99]\ttrain-logloss:0.02829\teval-logloss:0.25893\n",
      "[100]\ttrain-logloss:0.02789\teval-logloss:0.25928\n",
      "[101]\ttrain-logloss:0.02751\teval-logloss:0.25955\n",
      "[102]\ttrain-logloss:0.02714\teval-logloss:0.25901\n",
      "[103]\ttrain-logloss:0.02668\teval-logloss:0.25991\n",
      "[104]\ttrain-logloss:0.02634\teval-logloss:0.25950\n",
      "[105]\ttrain-logloss:0.02594\teval-logloss:0.25924\n",
      "[106]\ttrain-logloss:0.02556\teval-logloss:0.25901\n",
      "[107]\ttrain-logloss:0.02522\teval-logloss:0.25738\n",
      "[108]\ttrain-logloss:0.02492\teval-logloss:0.25702\n",
      "[109]\ttrain-logloss:0.02453\teval-logloss:0.25789\n",
      "[110]\ttrain-logloss:0.02418\teval-logloss:0.25770\n",
      "[111]\ttrain-logloss:0.02384\teval-logloss:0.25842\n",
      "[112]\ttrain-logloss:0.02356\teval-logloss:0.25810\n",
      "[113]\ttrain-logloss:0.02322\teval-logloss:0.25848\n",
      "[114]\ttrain-logloss:0.02290\teval-logloss:0.25833\n",
      "[115]\ttrain-logloss:0.02260\teval-logloss:0.25820\n",
      "[116]\ttrain-logloss:0.02229\teval-logloss:0.25905\n",
      "[117]\ttrain-logloss:0.02204\teval-logloss:0.25878\n",
      "[118]\ttrain-logloss:0.02176\teval-logloss:0.25728\n",
      "[119]\ttrain-logloss:0.02149\teval-logloss:0.25722\n",
      "[120]\ttrain-logloss:0.02119\teval-logloss:0.25764\n",
      "[121]\ttrain-logloss:0.02095\teval-logloss:0.25761\n",
      "[122]\ttrain-logloss:0.02067\teval-logloss:0.25832\n",
      "[123]\ttrain-logloss:0.02045\teval-logloss:0.25808\n",
      "[124]\ttrain-logloss:0.02023\teval-logloss:0.25855\n",
      "[125]\ttrain-logloss:0.01998\teval-logloss:0.25714\n",
      "[126]\ttrain-logloss:0.01973\teval-logloss:0.25587\n",
      "[127]\ttrain-logloss:0.01946\teval-logloss:0.25640\n",
      "[128]\ttrain-logloss:0.01927\teval-logloss:0.25685\n",
      "[129]\ttrain-logloss:0.01908\teval-logloss:0.25665\n",
      "[130]\ttrain-logloss:0.01886\teval-logloss:0.25712\n",
      "[131]\ttrain-logloss:0.01863\teval-logloss:0.25609\n",
      "[132]\ttrain-logloss:0.01839\teval-logloss:0.25649\n",
      "[133]\ttrain-logloss:0.01816\teval-logloss:0.25789\n",
      "[134]\ttrain-logloss:0.01802\teval-logloss:0.25811\n",
      "[135]\ttrain-logloss:0.01785\teval-logloss:0.25794\n",
      "[136]\ttrain-logloss:0.01763\teval-logloss:0.25876\n",
      "[137]\ttrain-logloss:0.01748\teval-logloss:0.25884\n",
      "[138]\ttrain-logloss:0.01732\teval-logloss:0.25867\n",
      "[139]\ttrain-logloss:0.01719\teval-logloss:0.25876\n",
      "[140]\ttrain-logloss:0.01696\teval-logloss:0.25987\n",
      "[141]\ttrain-logloss:0.01681\teval-logloss:0.25960\n",
      "[142]\ttrain-logloss:0.01669\teval-logloss:0.25982\n",
      "[143]\ttrain-logloss:0.01656\teval-logloss:0.25992\n",
      "[144]\ttrain-logloss:0.01638\teval-logloss:0.26035\n",
      "[145]\ttrain-logloss:0.01623\teval-logloss:0.26055\n",
      "[146]\ttrain-logloss:0.01606\teval-logloss:0.26092\n",
      "[147]\ttrain-logloss:0.01589\teval-logloss:0.26137\n",
      "[148]\ttrain-logloss:0.01572\teval-logloss:0.25999\n",
      "[149]\ttrain-logloss:0.01556\teval-logloss:0.26028\n",
      "[150]\ttrain-logloss:0.01546\teval-logloss:0.26048\n",
      "[151]\ttrain-logloss:0.01531\teval-logloss:0.26142\n",
      "[152]\ttrain-logloss:0.01515\teval-logloss:0.26188\n",
      "[153]\ttrain-logloss:0.01501\teval-logloss:0.26227\n",
      "[154]\ttrain-logloss:0.01486\teval-logloss:0.26287\n",
      "[155]\ttrain-logloss:0.01476\teval-logloss:0.26299\n",
      "[156]\ttrain-logloss:0.01462\teval-logloss:0.26346\n",
      "[157]\ttrain-logloss:0.01448\teval-logloss:0.26379\n",
      "[158]\ttrain-logloss:0.01434\teval-logloss:0.26306\n",
      "[159]\ttrain-logloss:0.01424\teval-logloss:0.26237\n",
      "[160]\ttrain-logloss:0.01410\teval-logloss:0.26251\n",
      "[161]\ttrain-logloss:0.01401\teval-logloss:0.26265\n",
      "[162]\ttrain-logloss:0.01392\teval-logloss:0.26264\n",
      "[163]\ttrain-logloss:0.01380\teval-logloss:0.26250\n",
      "[164]\ttrain-logloss:0.01372\teval-logloss:0.26264\n",
      "[165]\ttrain-logloss:0.01359\teval-logloss:0.26255\n",
      "[166]\ttrain-logloss:0.01350\teval-logloss:0.26188\n",
      "[167]\ttrain-logloss:0.01342\teval-logloss:0.26203\n",
      "[168]\ttrain-logloss:0.01331\teval-logloss:0.26190\n",
      "[169]\ttrain-logloss:0.01319\teval-logloss:0.26184\n",
      "[170]\ttrain-logloss:0.01312\teval-logloss:0.26133\n",
      "[171]\ttrain-logloss:0.01304\teval-logloss:0.26148\n",
      "[172]\ttrain-logloss:0.01297\teval-logloss:0.26157\n",
      "[173]\ttrain-logloss:0.01285\teval-logloss:0.26253\n",
      "[174]\ttrain-logloss:0.01278\teval-logloss:0.26229\n",
      "[175]\ttrain-logloss:0.01267\teval-logloss:0.26086\n",
      "[176]\ttrain-logloss:0.01258\teval-logloss:0.26103\n"
     ]
    }
   ],
   "source": [
    "params = {'max_depth':3,\n",
    "        'eta': 0.05,\n",
    "        'objective':'binary:logistic',\n",
    "        'eval_metric':'logloss'\n",
    "        }\n",
    "\n",
    "num_rounds = 400\n",
    "\n",
    "# 학습 데이터 셋은 'train' 또는 평가 데이터 셋은 'eval' 로 명기합니다. \n",
    "eval_list = [(dtr,'train'), (dval,'eval')] # 또는 eval_list = [(dval,'eval')] 만 명기해도 무방. \n",
    "\n",
    "# 하이퍼 파라미터와 early stopping 파라미터를 train( ) 함수의 파라미터로 전달\n",
    "xgb_model = xgb.train(params = params , dtrain=dtr , num_boost_round=num_rounds , \\\n",
    "                    early_stopping_rounds=50, evals=eval_list )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probs = xgb_model.predict(dtest)\n",
    "print('predict( ) 수행 결과값을 10개만 표시, 예측 확률 값으로 표시됨')\n",
    "print(np.round(pred_probs[:10],3))\n",
    "\n",
    "# 예측 확률이 0.5 보다 크면 1 , 그렇지 않으면 0 으로 예측값 결정하여 List 객체인 preds에 저장 \n",
    "preds = [ 1 if x > 0.5 else 0 for x in pred_probs ]\n",
    "print('예측값 10개만 표시:',preds[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "def get_clf_eval(y_test, pred=None, pred_proba=None):\n",
    "    confusion = confusion_matrix( y_test, pred)\n",
    "    accuracy = accuracy_score(y_test , pred)\n",
    "    precision = precision_score(y_test , pred)\n",
    "    recall = recall_score(y_test , pred)\n",
    "    f1 = f1_score(y_test,pred)\n",
    "    # ROC-AUC 추가 \n",
    "    roc_auc = roc_auc_score(y_test, pred_proba)\n",
    "    print('오차 행렬')\n",
    "    print(confusion)\n",
    "    # ROC-AUC print 추가\n",
    "    print('정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f},\\\n",
    "    F1: {3:.4f}, AUC:{4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_clf_eval(y_test , preds, pred_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 12))\n",
    "plot_importance(xgb_model, ax=ax)\n",
    "plt.savefig('p239_xgb_feature_importance.tif', format='tif', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 사이킷런 래퍼 XGBoost의 개요 및 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mxgboost\u001b[39;00m \u001b[39mimport\u001b[39;00m XGBClassifier\n\u001b[1;32m      4\u001b[0m xgb_wrapper \u001b[39m=\u001b[39m XGBClassifier(n_estimators\u001b[39m=\u001b[39m\u001b[39m400\u001b[39m, learning_rate\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m, max_depth\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m xgb_wrapper\u001b[39m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m      6\u001b[0m w_preds \u001b[39m=\u001b[39m xgb_wrapper\u001b[39m.\u001b[39mpredict(X_test)\n\u001b[1;32m      7\u001b[0m w_pred_proba \u001b[39m=\u001b[39m xgb_wrapper\u001b[39m.\u001b[39mpredict_proba(X_test)[:, \u001b[39m1\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# 사이킷런 래퍼 XGBoost 클래스인 XGBClassifier 임포트\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_wrapper = XGBClassifier(n_estimators=400, learning_rate=0.1, max_depth=3)\n",
    "xgb_wrapper.fit(X_train, y_train)\n",
    "w_preds = xgb_wrapper.predict(X_test)\n",
    "w_pred_proba = xgb_wrapper.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_clf_eval(y_test , w_preds, w_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_wrapper = XGBClassifier(n_estimators=400, learning_rate=0.1, max_depth=3)\n",
    "evals = [(X_test, y_test)]\n",
    "xgb_wrapper.fit(X_train, y_train, early_stopping_rounds=100, eval_metric=\"logloss\", \n",
    "                eval_set=evals, verbose=True)\n",
    "\n",
    "ws100_preds = xgb_wrapper.predict(X_test)\n",
    "ws100_pred_proba = xgb_wrapper.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_clf_eval(y_test , ws100_preds, ws100_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# early_stopping_rounds를 10으로 설정하고 재 학습. \n",
    "xgb_wrapper.fit(X_train, y_train, early_stopping_rounds=10, \n",
    "                eval_metric=\"logloss\", eval_set=evals,verbose=True)\n",
    "\n",
    "ws10_preds = xgb_wrapper.predict(X_test)\n",
    "ws10_pred_proba = xgb_wrapper.predict_proba(X_test)[:, 1]\n",
    "get_clf_eval(y_test , ws10_preds, ws10_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_importance\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 12))\n",
    "# 사이킷런 래퍼 클래스를 입력해도 무방. \n",
    "plot_importance(xgb_wrapper, ax=ax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "camp",
   "language": "python",
   "name": "camp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
