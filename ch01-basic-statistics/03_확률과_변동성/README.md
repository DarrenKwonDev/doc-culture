<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

- [01 확률이란 무엇인가](#01-확률이란-무엇인가)
  - [확률의 일반화된 덧셈법칙, 배반](#확률의-일반화된-덧셈법칙-배반)
  - [조건부 법칙과 곱셈법칙, 독립](#조건부-법칙과-곱셈법칙-독립)
  - [MECE한 partition(분할)](#mece한-partition분할)
  - [Naive Bayesian](#naive-bayesian)
  - [일반화된 Naive Bayesian](#일반화된-naive-bayesian)
  - [독립(independent)과 배반(mutually exclusive)](#독립independent과-배반mutually-exclusive)
  - [배반 -\> 독립, 독립 -\> 배반 두 명제 모두 거짓](#배반---독립-독립---배반-두-명제-모두-거짓)
- [02 binomial formula (이항공식)](#02-binomial-formula-이항공식)
  - [이항 공식](#이항-공식)
  - [이항확률변수 X의 기대값과 분산, 표준오차](#이항확률변수-x의-기대값과-분산-표준오차)
- [03 law of average (law of large number)](#03-law-of-average-law-of-large-number)
  - [box model(상자 모델)](#box-model상자-모델)
- [04 기대값 E(x)과 표준오차(SE, standard error)](#04-기대값-ex과-표준오차se-standard-error)
  - [표준오차 공식에 기반하여 평균의 법칙 설명하기](#표준오차-공식에-기반하여-평균의-법칙-설명하기)
  - [정규분포곡선을 활용한 확률 분포](#정규분포곡선을-활용한-확률-분포)
  - [box model(상자 모델)에서의 간단한 표준 편차 계산](#box-model상자-모델에서의-간단한-표준-편차-계산)
  - [기대값과 분산과 관련된 공식](#기대값과-분산과-관련된-공식)
- [05 정규분포곡선과 확률히스토그램](#05-정규분포곡선과-확률히스토그램)
  - [empirical histogram(경험적 히스토그램) vs probability histogram(확률적 히스토그램)](#empirical-histogram경험적-히스토그램-vs-probability-histogram확률적-히스토그램)
  - [central limit theorem(중심극한정리, CLT)](#central-limit-theorem중심극한정리-clt)
  - [정규 분포로의 근사와 연속성 수정(continuity correction)](#정규-분포로의-근사와-연속성-수정continuity-correction)
  - [경험 히스토그램 -\> 확률 히스토그램 -\> 정규분포로의 수렴](#경험-히스토그램---확률-히스토그램---정규분포로의-수렴)
  - [bootstrapping method(부트스트래핑)](#bootstrapping-method부트스트래핑)
- [glossary](#glossary)

<!-- /code_chunk_output -->

## 01 확률이란 무엇인가

확률은 크게 두 가지 관점이 있다.

1. 하나의 시행을 동일 조건 하 독립 시행으로 무한히 반복해서 측정하는 도수 이론 내지는 frequentism(빈도 주의)
2. 인간의 주관적인 감각 내지는 직관으로 얻는 주관적 확률 (subjective view)
3. 새로운 정보를 토대로 어떤 사건이 발생했다는 주장에 대한 신뢰도를 갱신해 나가는 방법으로서의 통계 (Bayesian view)
   - 이는 연역적인 빈도 주의가 아닌 통계를 귀납적으로 바라보는 패러다임 전환을 의미한다.

### 확률의 일반화된 덧셈법칙, 배반

$$
P(A \cup B) = P(A) + P(B) - P(A \cap B)
$$

여기서 A와 B가 상호 배반적인 사건(disjoint, mutex)일 경우 $P(A \cap B) = \emptyset$ 이므로 아래처럼 정리할 수 있다. 아래 섹션에서 좀 더 자세히 설명한다.

$$
P(A \cup B) = P(A) + P(B)
$$

### 조건부 법칙과 곱셈법칙, 독립

$$
P(A \mid B) = \frac{P(A \cap B)}{P(B)} \\
P(A \cap B) = P(A)P(B \mid A) = P(B)P(A \mid B)
$$

후술하지만, 독립이라면 $P(A \mid B) = P(A)$ 이므로 아래처럼 정리할 수 있다. 아래 섹션에서 좀 더 자세히 설명한다.

$$
P(A \mid B) = P(A)P(B)
$$

### MECE한 partition(분할)

사건이 전체를 포괄하고(collectively exhaustive) 상호 배반적(mutual exclusive)일 때,
컨설팅 업계에서는 흔히 이를 'MECE'하다라고 표현한다. 통계학에서는 이를 전체를 '분할'(partition)한다고 표현한다.
통계학에서 MECE가 중요한 이유는 여기에 기반하여 베이즈 정리를 이해할 수 있기 때문이다.

### Naive Bayesian

베이즈 정리에 이르는 정의는 생략하고, 단순한 형태의 베이즈 정리는 아래와 같다.
$P(A \mid B)$는 posterior(사후 확률), P(A)는 prior(사전 확률), $P(B \mid A)$는 likelihood(우도)라고 부른다.

$$
P(A \mid B) = \frac{P(B \mid A)P(A)}{P(B)} \\
P(가설|증거) = P(증거|가설)P(가설) / P(증거)
$$

문제는 이를 받아들이는 직관인데, `구하려는 사건과 주어진 사건의 순서를 뒤집어 계산할 수 있다는 것`이다.

### 일반화된 Naive Bayesian

특정 사건 S를 MECE하게 $A_1, A_2, \cdots, A_m$으로 분할(partition)했다고 하자.
여기서 우리는 S라는 공간 내에 B라는 새로운 사건이 발생했을 때, A_1이라는 사건이 발생할 확률을 구해낼 수 있습니다.

$$
P(A_1 \mid B) = \frac{P(B \mid A_1)}{P(B)} = \frac{P(A_1)P(B \mid A_1)}{P(A_1)P(B \mid A_1) + \cdot P(A_m)P(B \mid A_m)}
$$

- https://junpyopark.github.io/bayes/
- https://angeloyeo.github.io/2020/01/09/Bayes_rule.html

### 독립(independent)과 배반(mutually exclusive)

- A 이거나 B = or($ \cup $) 조건 = 덧셈 법칙 = 배반인지 고려해야 함
  - 배반 : $P(A) \cap P(B) = 0$ 즉, A와 B가 별개의 조건. 벤 다이어그램으로 표현하면 교집합이 없는 경우다.
  - 확률간 덧셈을 할 때 사건들이 배반인지 고려해야 함. 이는 본능적으로 이뤄져야 함.
- A 이고 B = and ($ \cap $) 조건 = 곱셈 법칙 = 독립인지 고려해야 함
  - 독립 : $P(A \mid B) = P(A)$ 즉, B가 일어나건 말건 A 확률은 그대로임. 풀어 쓰자면 A가 B 혹은 B가 A의 서로의 확률에 영향을 주지 않음.
  - 직관적으로 이해하면 두 사건은 독립이다 = 두 사건은 연관 없다로 이해하자.
  - 증명하려면 그냥 $P(A) \cap P(B) = P(A) \times P(B)$ 로 증명하는게 빠르다.
  - 확률간 곱셈을 할 떄 사건들이 독립인지 고려해야 함. 이는 본능적으로 이뤄져야 함.

배반

$$
P(A) \cap P(B) = 0
$$

독립

$$
  P(A \mid B) = P(A) \\
  P(B \mid A) = P(B) \\
  P(A \cap B) = P(A)P(B)
$$

### 배반 -> 독립, 독립 -> 배반 두 명제 모두 거짓

독립사건과 배반사건은 서로 전혀 연관이 없는 개념이다.

교집합이 있을 때(사건이 배반이 아니더라도) 사건들은 상호 독립적일 수 있다. 잘 생각해보면 상식적인 것이다. 주사위 짝수일 확률이 주사위 눈이 3이상일 확률과 무슨 상관이 있는가? 예시로 이해해보자면 https://kenadams.tistory.com/38 참고
따라서 `독립 -> 배반` 명제는 거짓이다.

또한 `배반 -> 독립`도 거짓이다. 배반 사건이라면 $P(A) \cap P(B) = 0$ 인데 독립을 증명하기 위한 공식인 $P(A \cap B) = P(A)P(B)$ 가 애초에 0이 되어버림. 독립 사건의 전제 조건은 적어도 각 A, B에 대한 확률이 0은 아니어야 함.
따라서 `배반 -> 독립` 명제는 거짓이다. 예시로 이해해보려면 https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=parkhc1992&logNo=220587985603 참고

## 02 binomial formula (이항공식)

[이산확률분포(discrete probability distribution)](https://namu.wiki/w/%ED%99%95%EB%A5%A0%EB%B6%84%ED%8F%AC)에는 이항분포, 푸아송분포, 기하 분포 등 다양한 분포가 존재한다. 여기서는 이항 분포를 다루는 이항공식에 대해 알아보자.

이항 분포를 따르는 이항확률 변수 X는 $X \sim B(n, p)$ 로 표기할 수 있다.

### 이항 공식

- n번의 독립시행 (독립이 아니라면 한 사건의 발생이 다른 사건에 영향을 미치므로 이항공식 적용 불가)
- 관심사건 A에 대해 P(A) = p이고, P(A') = 1-p (베르누이 시행)
- n번중 A가 k번 발생

일 때 아래와 같이 이항 공식이 적용된다.

$$
P(X=k) = _{n}C_{k} p^k (1-p)^{n-k}
$$

또한, 이항 공식을 합하면 사건 전체의 확률이므로 1이 된다.

$$
\sum_{k=0}^{n} P(X=k) = 1
$$

### 이항확률변수 X의 기대값과 분산, 표준오차

$$
E(X) = np \\
Var(X) = np(1-p) \\
SE = \sqrt{Var(X)} = \sqrt{np(1-p)}
$$

## 03 law of average (law of large number)

평균의 법칙은 시행 횟수가 많아지면 평균에 수렴한다는 의미도 있지만, 더 중요한 것은  
`확률 오차(stochastic error)는 시행 횟수가 증가함에 따라 절대적으로는 커지지만 시행 횟수에 대비하면 상대적으로 적어지게 된`다는 점입니다.

### box model(상자 모델)

하나의 확률과정을 상자에서 숫자를 무작위로 추출하는 과정으로 묘사하는 것이다.  
예를 들어 주사위를 던져 홀수 나오면 1000원, 짝수 나오면 -1000원을 얻는 게임은, 상자에 1000원 카드 3개, -1000원 카드 3개를 넣고 무작위로 복원 추출을 하는 것과 같다는 모델이다.

해당 상자 모델로 문제를 변형하여 생각하는 것이 종종 도움이 된다.

## 04 기대값 E(x)과 표준오차(SE, standard error)

확률 분포 상의 중심값을 기대값(E), 그리고 기대값과 실제 차이가 나는 정도를 확률 오차(stochastic error)라 한다.  
이 확률 오차의 크기가 표준적으로 어느 정도가 되는지 나타내는 것이 표준오차 (SE, standard error)이다.
제곱근법칙(square root law)에 의해 표준 오차를 계산할 수 있다.

주의할 점으로, 상자의 표준 편차(SD)는 모표준편차이므로 자유도를 고려하지 않고 $\sqrt{\frac{\sum_{i=1}^{n}(X_{i}-\overline{X})^2}{n}}$ 로 계산합니다.

$$
SE = SD * \sqrt{n} \\
합의 SE = 상자 모델의 SD * \sqrt{n}
$$

위 식에서 직관적으로 알 수 있는 것은
(1) 상자 모델의 편차가 커질 수록 표준 오차가 커진다는 것
(2) 시행 횟수가 증가할 수록 표준 오차가 커진다는 것, 다만 선형적 증가가 아닌 $\sqrt{시행 횟수}$ 만큼 증가한다는 것이다.

### 표준오차 공식에 기반하여 평균의 법칙 설명하기

앞서 평균의 법칙의 핵심이 시행 횟수가 늘어남에 따라 확률 오차는 커지지만 상대적인 크기가 줄어든다는 것이었다.

이를 표준오차 공식에 기대어 확인하자면, 시행 확률이 커질때마다 SE가 커지지만, 시행 횟수가 커질수록 $\sqrt{시행 횟수}$ 비율 만큼 커지므로 시행횟수가 선형적으로 증가하지만 SE는 제곱근적으로 늘어나기 때문에 시행 횟수에 비해 SE의 상대적 크기는 줄어든다고 설명할 수 있다.

### 정규분포곡선을 활용한 확률 분포

**추출 횟수가 충분히 많은** **합**의 확률 분포(또는 합을 통해 도출하는 **평균**의 확률 분포)는 정규분포를 따른다고 수학적으로 알려져있다. (중심극한정리)  
(그렇다면 추출 횟수가 적다면 정규분포곡선을 따르지 않는다는 말인가? -> 그렇다. 정규분포를 따르지 않는다)  
(합이나 평균이 아닌 곱의 그래프틑 정규분포를 따르지 않는다.)  
그렇다면 관측된 합을 표준단위로 바꾼 후 정규 분포 곡선을 그려 활용할 수 있다.  
정규 분포이므로 정규 분포에서 $\pm 1, 2, 3SD$가 68-95-99.7 법칙을 따르듯, 확률 분포도 기대값을 중심으로 $\pm 1, 2, 3SE$ 가 68-95-99.7 법칙을 따릅니다.

과거 살펴본 표준화 공식은 아래와 같았다.

$$
\frac{x - \mu}{\sigma}
$$

우리는 합의 확률 분포를 다루고 있으므로 이를 표준화하면 아래와 같다. 기대값을 뺀 후 표준 오차로 나눈 것이다.

$$
\frac{x - E(x)}{SE}
$$

이러한 표준화를 통해 평균이 0, 표준편차가 1인 정규분포 곡선을 그릴 수 있다.  
이 곡선에 근거하여 표준확률분포표를 참고하여 해당 곡선의 넓이(확률)을 구할 수 있게 된다.

### box model(상자 모델)에서의 간단한 표준 편차 계산

상자 모델 내 카드(선택지)가 2종류 뿐이라면(2개가 아니라 2 '종류'임에 유의) SD를 빠르게 계산할 수 있다  
증명은 <통계학>(류근관 저), p.256 참고

$$
(큰수 - 작은 수) \times \sqrt{(큰 수의 비율) \times (작은 수의 비율)}
$$

### 기대값과 분산과 관련된 공식

$$
E(X + Y) = E(X) + E(Y) \\
E(aX + b) = aE(X) + b \\
Var(X) = E(X^2) - E(X)^2 \\
VAR(X + Y) = VAR(X) + VAR(Y) + 2COV(X, Y) \\
VAR(X + Y) = VAR(X) + VAR(Y) (X, Y가 독립일 때 공분산이 0이므로) \\
VAR(aX + b) = a^2VAR(X) \\
E(XY) = E(X)E(Y) (X, Y가 독립일 때)
$$

## 05 정규분포곡선과 확률히스토그램

### empirical histogram(경험적 히스토그램) vs probability histogram(확률적 히스토그램)

[ch01의 01 자료의 정리에서 살펴본 히스토그램](./../01_자료의_정리/README.md#02-히스토그램)은 자료의 빈도를 나타낸 empirical histogram이다. 실제로 자료를 측정한 값들의 빈도를 나타낸 것이다.

그러나 probability histogram은 상자 모형으로부터 계산된 합의 기대값과 표준오차에 기반하여 **합, 평균**이 특정한 값을 취할 확률을 알려주는 히스토그램이다. 따라서 모든 값을 측정하지 않는다.

다만 추출 횟수가 충분히 많아질 경우 경험적 히스토그램은 확률적 히스토그램에 수렴하게 된다.  
결국 경험적으로 히스토그램을 그리기보다 머리로 확률적 히스토그램을 그려서 계산을 할 수 있다는 것이다. 이것은 큰 장점이다.

### central limit theorem(중심극한정리, CLT)

앞서 추출 횟수가 충분히 많다면 합, 평균의 확률 분포가 정규분포를 따라간다고 적은 바 있다. 이는 상자의 내용물과 관계 없이 성립한다.
0이 9장, 9가 1장 들어 있는 상자마저 도달하는 속도는 느리더라도 추출 횟수가 충분히 많아진다면 정규 분포를 따르게 된다.

> **추출 횟수가 충분히 많은** **합**의 확률 분포(또는 합을 통해 도출하는 **평균**의 확률 분포)는 정규분포를 따른다고 수학적으로 알려져있다.  
> (그렇다면 추출 횟수가 적다면 정규분포곡선을 따르지 않는다는 말인가? -> 그렇다. 정규분포를 따르지 않는다)  
> (합이나 평균이 아닌 곱의 그래프틑 정규분포를 따르지 않는다.)

위 내용을 재정리하자면 중심극한정리는 합, 평균에 대해서 적용되며 곱에 대해서는 적용되지 않는다고 할 수 있다.

> 확률론과 통계학에서 중심 극한 정리(central limit theorem, 약자 CLT)는 동일한 확률분포를 가진 독립 확률 변수 n개의 평균의 분포는 n이 적당히 크다면 정규분포에 가까워진다는 정리이다.
> [위키피디아 출처](https://ko.wikipedia.org/wiki/%EC%A4%91%EC%8B%AC_%EA%B7%B9%ED%95%9C_%EC%A0%95%EB%A6%AC#:~:text=%ED%99%95%EB%A5%A0%EB%A1%A0%EA%B3%BC%20%ED%86%B5%EA%B3%84%ED%95%99%EC%97%90%EC%84%9C%20%EC%A4%91%EC%8B%AC,%EC%97%90%20%EA%B0%80%EA%B9%8C%EC%9B%8C%EC%A7%84%EB%8B%A4%EB%8A%94%20%EC%A0%95%EB%A6%AC%EC%9D%B4%EB%8B%A4.)

풀어쓰자면,  
(1) 동일한 확률 분포, (2) 각 시행이 독립 이라면 (3) 합 또는 평균의 분포가 정규분포가 된다는 것이다.

### 정규 분포로의 근사와 연속성 수정(continuity correction)

CLT 법칙에 의하여 경험이 아닌 '머리로' 작성된 확률적 히스토그램은 시행횟수가 충분히 크면 정규 분포에 가까워지므로 이들을 계산할 때 정규 분포를 사용하여 값을 근사할 수 있다.

그런데 이러한 근사에서 주의해야 할 점은, 확률로 얻어지는 히스토그램은 이산적(discrete)이지만, 정규 분포는 연속적(continuous)이라는 점이다. 그래서 오차를 일부 허용 방식으로 근사 과정을 진행한다 (<통계학> 류근관, p.275 참고)

예를 들어 100번 동전을 던져서 정확히 50번이 나올 확률을 근사하는 과정을 보자면 49.5 부터 50.5까지의 구간에 대응하는 '면적'으로써 계산해야 한다. 이러한 계산 방법을 연속성 수정(continuity correction)이라 한다.

연속성 수정(continuity correction)의 다른 예시로, 앞면이 45번 초과, 55번 미만일 확률을 구하려고 하더라도 '초과', '미만'이므로 45.5 부터 54.5까지의 구간에 대응하는 '면적'으로써 계산해야 한다.

약간 다르게 45번 이상, 55 이하일 확률을 구하려면 44.5 부터 55.5까지의 구간에 대응하게 된다. 이로써 '초과/이상', '미만/이하' 워딩에 의하여 근사되는 확률에서 차이가 나게 되는 것이다.

### 경험 히스토그램 -> 확률 히스토그램 -> 정규분포로의 수렴

앞서 살펴본 내용을 정리하자면 두 가지 수렴이 존재한다. 이 둘을 명확히 구분하는 것이 중요하기 때문에 별도의 섹션을 분리하였다.  
(1) 경험적 히스토그램이 확률적 히스토그램에 수렴하는 것은 평균(대수)의 법칙 때문이다. 충분히 많은 데이터가 쌓이면 경험적으로 그린 것이 확률적으로 그린 것에 수렴한다.  
(2) 확률적 히스토그램이 정규분포에 수렴하는 것은 중심극한정리 때문이다. 충분히 많은 데이터가 쌓이면 확률적으로 그린 것이 정규분포에 수렴하므로 정규 분포를 활용한 계산이 가능하다.

### bootstrapping method(부트스트래핑)

추출 횟수 자체가 크고 데이터 자체가 커야 정규 분포로의 수렴이 가능하다. 그러나 현실에서는 데이터가 얼마 없는 경우가 대부분이다.  
이런 경우에는 부트스트래핑(bootstrapping)을 활용한다. 부트스트래핑은 데이터를 재추출하는 방법으로, 데이터를 재추출하는 횟수를 늘리면 정규 분포로의 수렴이 가능해진다. 부트스트래핑은 다음과 같은 과정을 거친다.

(1) 원래의 표본으로부터 복원추출로 같은 크기의 bootstrap sample을 만든다.  
(2) bootstrap sample을 이용하여 통계량을 계산한다.  
(3) (1)과 (2)를 원하는 만큼 반복한다.  
(4) (3)에서 계산된 통계량의 분포를 이용하여 원래의 표본에 대한 통계량의 신뢰구간을 구한다.

## glossary

- 배반과 독립
- 복원추출과 비복원추출
- 조건부확률과 비조건부확률
- 표준 오차
